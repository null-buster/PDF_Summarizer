{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5 Abstractive Summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlbMpHMFVwWXCDwODD/aAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/null-buster/PDF_Summarizer/blob/main/T5_Abstractive_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqrauj0L_r6F",
        "outputId": "f2c7016a-0369-4fd0-d112-b2a047aebe20"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentencepiece\n",
        "!pip install pdfminer.six"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.6/dist-packages (20201018)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (2.3.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.2.1)\n",
            "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->pdfminer.six) (1.14.3)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->pdfminer.six) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfPbtSMBAhhW"
      },
      "source": [
        "Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iik9Td4u_8Yb"
      },
      "source": [
        "import torch\n",
        "import json \n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "#import pdfminer\n",
        "import os\n",
        "import io\n",
        "import io\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.pdfinterp import PDFResourceManager\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "#print(pdfminer.__version__)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-F3p03IBxEQ"
      },
      "source": [
        "def pre_process_text(text):\n",
        "  preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
        "  t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "  #print (\"original text preprocessed: \\n\", preprocess_text)\n",
        "  return preprocess_text,t5_prepared_Text"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxbr87z6B2mo"
      },
      "source": [
        "def get_summary(t5_prepared_Text):\n",
        "  tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
        "  \n",
        "  # summmarize \n",
        "  summary_ids = model.generate(tokenized_text,\n",
        "                                      num_beams=4,\n",
        "                                      no_repeat_ngram_size=2,\n",
        "                                      min_length=30,\n",
        "                                      max_length=100,\n",
        "                                      early_stopping=True)\n",
        "  output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "  return output\n",
        "  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h2vBZmTB9e3",
        "outputId": "dd67cad5-6133-4eb4-8120-f4ae7aa15b46"
      },
      "source": [
        "#using the t5-base pretrained checkpoint trained on 220 million parameters\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "device = torch.device('cpu')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
            "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQjyiiaVLhYo"
      },
      "source": [
        "def extract_text_by_page(pdf_path):\n",
        "    with open(pdf_path, 'rb') as fh:\n",
        "        for page in PDFPage.get_pages(fh, \n",
        "                                      caching=True,\n",
        "                                      check_extractable=True):\n",
        "            resource_manager = PDFResourceManager()\n",
        "            fake_file_handle = io.StringIO()\n",
        "            converter = TextConverter(resource_manager, fake_file_handle)\n",
        "            page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
        "            page_interpreter.process_page(page)\n",
        "            \n",
        "            text = fake_file_handle.getvalue()\n",
        "            yield text\n",
        "    \n",
        "            # close open handles\n",
        "            converter.close()\n",
        "            fake_file_handle.close()\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHnSfdKzLj7j"
      },
      "source": [
        "def extract_text(pdf_path):\n",
        "  final_summary = \"\"\n",
        "  page_count = 1\n",
        "  for page in extract_text_by_page(pdf_path):\n",
        "    #print(page)\n",
        "    #print(type(page))\n",
        "    text = page\n",
        "    preprocess_text, t5_prepared_Text = pre_process_text(text)\n",
        "    #print (\"original text preprocessed: \\n\", preprocess_text)\n",
        "    output = get_summary(t5_prepared_Text)\n",
        "    #print (\"\\n\\nSummarized text: \\n\",output)\n",
        "    print()\n",
        "    final_summary += output\n",
        "    #print(final_summary)\n",
        "    final_summary += \"\\n\\n\"\n",
        "    print(\"Done Summarizing Page----------->>> \" + str(page_count))\n",
        "    page_count += 1\n",
        "  print(\"\\n\\nSummarization done. Printing summary.\\n\")\n",
        "  return final_summary\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjyH_3UuaX0F"
      },
      "source": [
        "def read_pdf_paths(folder_name, output_folder):\n",
        "  path_to_files = os.listdir(os.path.abspath('.') + '/' + folder_name)\n",
        "  for entry in path_to_files:\n",
        "    if entry.split('.')[-1] == 'pdf':\n",
        "      print(\"Processing Pdf : \" + entry)\n",
        "      final_summary = extract_text(os.path.abspath('.') + '/' + folder_name + '/' + entry)\n",
        "      with open(os.path.abspath('.') + '/' + output_folder + '/Summary_' + entry.split('.')[-2] + '.txt', 'w' ) as f:\n",
        "        f.write(\"Summary of \" + entry.split('.')[-2]+\":\\n\\n\" )\n",
        "        f.write(\"\\n\\n===========================================\\n\\n\" )\n",
        "        f.write(final_summary)\n",
        "      #print(entry)\n",
        "      print(\"Summary of \" + entry.split('.')[-2]+\" is :\\n\\n\")\n",
        "      print(final_summary)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU2vi-SmxG9m"
      },
      "source": [
        "### Output: \n",
        "The code below created the input and outputs folders for the model to take inputs from and then reads each pdf page by page to extract summaries and then saves the summaries as plain text files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPLH1tNZM-WO"
      },
      "source": [
        "Steps for running the script:\n",
        "\n",
        "1. Run all the cells above this point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfUVujYk6z4w"
      },
      "source": [
        "\n",
        "2.   Run the cell below to create the input folder containing the Pdfs with the name `pdf_folder` and the output folder that'll contain the output summaries as text files with the name `summary_folder`.\n",
        "\n",
        "Note : This step could be skipped if these folders are already present or if you wish to manually create these folders.\n",
        "\n",
        "3.   Populate the `pdf_folder` with the Pdfs to be summarized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO9dJg-2yKPv"
      },
      "source": [
        "pdf_folder = '/pdf_folder'\n",
        "if not os.path.exists(os.path.abspath('.') + pdf_folder):\n",
        "  os.mkdir(os.path.abspath('.') + pdf_folder)\n",
        "summary_folder = '/summary_folder'\n",
        "if not os.path.exists(os.path.abspath('.') + summary_folder):\n",
        "  os.mkdir(os.path.abspath('.') + summary_folder)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q19kaCQK796n"
      },
      "source": [
        "\n",
        "\n",
        "3.   Run the code below with the appropriate folder names created in the previous step. The program will read all the Pdfs present in the `pdf_folder` and output the results in the `summary_folder`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-viJH9F8byzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605b411c-fc99-4838-b56a-ca110a300a51"
      },
      "source": [
        "read_pdf_paths('pdf_folder', 'summary_folder')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Pdf : The-COVID-19-recovery-will-be-digital-A-plan-for-the-first-90-days-vF_677b26fd2626e24c8ab1bf28ed70410e.pdf\n",
            "\n",
            "Done Summarizing Page----------->>> 1\n",
            "\n",
            "Done Summarizing Page----------->>> 2\n",
            "\n",
            "Done Summarizing Page----------->>> 3\n",
            "\n",
            "Done Summarizing Page----------->>> 4\n",
            "\n",
            "Done Summarizing Page----------->>> 5\n",
            "\n",
            "Done Summarizing Page----------->>> 6\n",
            "\n",
            "Done Summarizing Page----------->>> 7\n",
            "\n",
            "Done Summarizing Page----------->>> 8\n",
            "\n",
            "\n",
            "Summarization done. Printing summary.\n",
            "\n",
            "Summary of The-COVID-19-recovery-will-be-digital-A-plan-for-the-first-90-days-vF_677b26fd2626e24c8ab1bf28ed70410e is :\n",
            "\n",
            "\n",
            "COVID-19 recovery will be digital: A plan for the first 90 days. rapid migration to digital technologies driven by the pandemic will continue into the recovery.\n",
            "\n",
            "digital will undoubtably play a center-stage role during the recovery and beyond. banks have transitioned to remote sales and service teams, schools have pivoted to online learning and digital classrooms, doctors have begun delivering telemedicine, aided by more flexible regulation. fully 75 percent of people using digital channels for the first time indicate they will continue to use them when things return to “normal.”\n",
            "\n",
            "consumers are accelerating adoption of digital channels, a trend seen across global regions. consumption patterns will be uneven and unlikely to return to pre-COVID-19 levels quickly. levels of remote working have skyrocketed during lockdowns and are likely to remain higher than precrisis levels for some time.\n",
            "\n",
            "the digital agenda for recovery focuses on four efforts: refocusing and accelerating digital investments. using new data and AI to improve business operations, selectively modernizing technology capabilities to boost development velocity, and increasing organizational agility to deliver more quickly. for each one, we outline a practical 90-day plan to make it happen.\n",
            "\n",
            "airlines are rapidly reinventing the passenger experience with contactless journeys. a renewed digital agenda should take no longer than 30 days to establish, says robert mcdonald jr. the travel industry is mapping out the customer journey to identify points of health risk.\n",
            "\n",
            "many companies have the potential to free up as much as 45 percent of their IT costs over the course of a year. the good news is that cloud technologies make it possible to deploy these quickly and at relatively low cost.\n",
            "\n",
            "companies that have led the way in adopting flatter, fully agile organizational models have shown substantial improvements in both execution pace and productivity. remote working can also help organizations move at a faster clip as companies tap into new labor pools and specialized remote expertise.\n",
            "\n",
            "the 90-day plan will help organizations get there. achieving parity or better across digital channels to win the revenue race, rebuilding the most critical decision-support models, and doubling development velocity are goals that are all within reach.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}